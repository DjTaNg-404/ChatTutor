from typing import Dict, Any, Literal

from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from rich import print as rprint
from rich.panel import Panel

from langchain_core.messages import ToolMessage

from app.core.config import settings
from app.core.models import AgentState, ExecutionPlan
from app.core import prompts, memory, context
from app.core.tools import search_tool

# --- 1. Model Initialization ---
# é€šç”¨æ¨¡å‹ï¼šç”¨äºæ–‡æœ¬ç”Ÿæˆ (Tutor, Judge, Inquiry, Aggregator)
model = ChatDeepSeek(
    model=settings.MODEL_NAME,
    api_key=settings.DEEPSEEK_API_KEY,
    temperature=0.7
)

# ç»‘å®šäº†å·¥å…·çš„æ¨¡å‹ (ReAct Worker)
# DeepSeek åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬æŠŠ Search ç»‘å®šç»™å®ƒ
model_with_tools = model.bind_tools([search_tool])

# åˆ†ææ¨¡å‹ï¼šç”¨äºç»“æ„åŒ–è¾“å‡ºè§„åˆ’ (Analyzer)
# é€šå¸¸æˆ‘ä»¬å¯ä»¥ç”¨ç¨å¾®ä½ä¸€ç‚¹çš„ temperature ä¿è¯ JSON æ ¼å¼ç¨³å®š
analyzer_model_raw = ChatDeepSeek(
    model=settings.MODEL_NAME, # æˆ–è€…ä½¿ç”¨æ›´ä¾¿å®œ/å¿«é€Ÿçš„æ¨¡å‹
    api_key=settings.DEEPSEEK_API_KEY,
    temperature=0.1
)

# --- 2. Node Functions (èŠ‚ç‚¹é€»è¾‘) ---

def analyzer_node(state: AgentState) -> Dict[str, Any]:
    """
    å¤§è„‘èŠ‚ç‚¹ï¼šåˆ†æç”¨æˆ·æ„å›¾å¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’ (ExecutionPlan)ã€‚
    åŒæ—¶è´Ÿè´£æ¸…ç†ä¸Šä¸€è½®çš„ä¸´æ—¶è¾“å‡ºã€‚
    """
    messages = state["messages"]
    if not messages:
        return {}
    
    recent_context = messages[-3:]
    
    # æ„é€  Prompt
    sys_msg = SystemMessage(content=prompts.ANALYZER_SYSTEM_PROMPT)
    
    # ç»‘å®šç»“æ„åŒ–è¾“å‡º
    planner = analyzer_model_raw.with_structured_output(ExecutionPlan)
    
    try:
        # Analyzer ä¹Ÿåº”è¯¥çœ‹åˆ°ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œå¦åˆ™å®ƒå¯èƒ½å¬ä¸æ‡‚å…³äºæ—§è¯é¢˜çš„å›ç­”
        # ä¸è¿‡ä¸ºäº†ç®€å•å‡†ç¡®ï¼ŒæŠŠ Summary æ”¾åœ¨ System Prompt ä¹‹åæ¯”è¾ƒå¥½
        inputs = [sys_msg]
        if state.get("conversation_summary"):
             inputs.append(SystemMessage(content=f"Context: {state.get('conversation_summary')}"))
        
        inputs.extend(recent_context)
        
        plan: ExecutionPlan = planner.invoke(inputs)
    except Exception as e:
        # Fallback ç­–ç•¥ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤å½“ä½œæ™®é€šæé—®
        print(f"Analyzer Error: {e}")
        plan = ExecutionPlan(
            needs_tutor_answer=True,
            needs_judge=False,
            needs_inquiry=False,
            thought_process="Error in planning, defaulting to simple answer."
        )
    
    # è¿”å›è®¡åˆ’ï¼Œå¹¶é‡ç½®ä¸´æ—¶å­—æ®µï¼Œé˜²æ­¢æ±¡æŸ“
    return {
        "plan": plan,
        "tutor_output": None,
        "judge_output": None,
        "inquiry_output": None,
        "summary_output": None
    }

def _run_tool_loop(prompt_content, state):
    """
    å…·ä½“çš„ ReAct å¾ªç¯é€»è¾‘ï¼š
    1. è°ƒç”¨æ¨¡å‹ (å¸¦å·¥å…·)
    2. å¦‚æœæœ‰ tool_calls -> æ‰§è¡Œå·¥å…· -> å°†ç»“æœè¿½åŠ åˆ°ä¸´æ—¶æ¶ˆæ¯ -> å†æ¬¡è°ƒç”¨æ¨¡å‹
    3. å¦‚æœæ²¡æœ‰ -> ç›´æ¥è¿”å›ç»“æœ
    """
    
    # ä½¿ç”¨ context.build_context æ„é€ åŒ…å« Summary + Recent Window çš„æ¶ˆæ¯åˆ—è¡¨
    # æ³¨æ„ï¼šbuild_context è¿”å›çš„æ˜¯ [System, Summary?, RecentMessages...]
    # prompt_content è¿™é‡Œæ˜¯ system prompt æ­£æ–‡
    current_messages = context.build_context(state, prompt_content)

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè®©æ¨¡å‹æ€è€ƒæ˜¯å¦éœ€è¦å·¥å…·
    # æ³¨æ„è¿™é‡Œä½¿ç”¨çš„æ˜¯ runnable listï¼Œè€Œä¸æ˜¯ state["messages"] å…¨é›†
    response = model_with_tools.invoke(current_messages)

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚
    if response.tool_calls:
        # æ‰§è¡Œæ‰€æœ‰è¯·æ±‚çš„å·¥å…·
        for tool_call in response.tool_calls:
            # è¿™é‡Œç®€å•èµ·è§åªå¤„ç† search_toolï¼Œæœªæ¥å¯èƒ½æœ‰æ›´å¤š
            if tool_call["name"] == "baidu_search":
                args = tool_call['args']
                query_str = args.get('query', str(args))
                # ä½¿ç”¨ rich æ‰“å°æ¼‚äº®çš„æœç´¢çŠ¶æ€
                rprint(f"[dim italic]   ğŸ” Searching Baidu for: [cyan]{query_str}[/cyan] ...[/dim italic]")
                
                tool_result = search_tool.invoke(tool_call["args"])
                
                # æ„é€  ToolMessage åé¦ˆç»™æ¨¡å‹
                tool_msg = ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=str(tool_result),
                    name=tool_call["name"]
                )
                current_messages.append(response) # æŠŠå¸¦æœ‰ tool_call çš„ AIMessage ä¹ŸåŠ ä¸Š
                current_messages.append(tool_msg) # æŠŠ ToolMessage åŠ ä¸Š
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šåŸºäºå·¥å…·ç»“æœç”Ÿæˆå›ç­”
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè¦æœ€ç»ˆç»“æœï¼Œä¸å†ç»‘å®šå·¥å…·ï¼Œé˜²æ­¢å®ƒæ­»å¾ªç¯ä¸€ç›´æœ
        final_response = model.invoke(current_messages)
        return final_response.content
    else:
        # æ²¡ç”¨å·¥å…·ï¼Œç›´æ¥è¿”å›
        return response.content

def tutor_node(state: AgentState) -> Dict[str, Any]:
    """
    Worker A: ç­”ç–‘è€…ã€‚æ”¯æŒè”ç½‘æœç´¢ã€‚
    """
    topic = state.get("current_topic", "General Knowledge")
    prompt_str = prompts.TUTOR_WORKER_PROMPT.format(topic=topic)
    
    content = _run_tool_loop(prompt_str, state)
    
    return {"tutor_output": content}

def judge_node(state: AgentState) -> Dict[str, Any]:
    """
    Worker B: è¯„å®¡å‘˜ã€‚æ”¯æŒè”ç½‘æœç´¢ã€‚
    """
    topic = state.get("current_topic", "General Knowledge")
    prompt_str = prompts.JUDGE_WORKER_PROMPT.format(topic=topic)
    
    content = _run_tool_loop(prompt_str, state)
    
    return {"judge_output": content}

def inquiry_node(state: AgentState) -> Dict[str, Any]:
    """
    Worker C: æ¢ç©¶è€…ã€‚æå‡ºå¯å‘å¼é—®é¢˜ã€‚
    """
    topic = state.get("current_topic", "General Knowledge")
    judge_fb = state.get("judge_output") or "æ— "
    
    # ç›´æ¥åœ¨ Prompt ä¸­æ³¨å…¥ Feedbackï¼Œç®€å•æ˜äº†
    sys_msg_str = prompts.INQUIRY_WORKER_PROMPT.format(topic=topic, judge_feedback=judge_fb)
    
    # å…ˆæ„å»ºæ ‡å‡†ä¸Šä¸‹æ–‡ [System, Summary, Recent]
    inputs = context.build_context(state, sys_msg_str)
    
    response = model.invoke(inputs)
    
    return {"inquiry_output": response.content}

def summary_node(state: AgentState) -> Dict[str, Any]:
    """
    Worker D: æ€»ç»“è€…ã€‚å…¼é¡¾ 'ä¸´æ—¶å›é¡¾' å’Œ 'ç¦»åœºç¬”è®°'ã€‚
    """
    plan = state.get("plan")
    
    # åœºæ™¯1ï¼šEnding (ç¦»åœº)
    if plan and plan.is_concluding:
        # 1. å¼ºåˆ¶ç”Ÿæˆä¸€ä»½å­¦ä¹ æŠ¥å‘Š (Note)
        # ä½¿ç”¨ä¸“é—¨çš„ç»“è¯¾æŠ¥å‘Š Prompt
        # å½“åšæ€»ç»“æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥çœ‹å°½å¯èƒ½å¤šçš„å†å²ï¼Œæ‰€ä»¥è¿™é‡Œä¹Ÿè®¸ä¸éœ€è¦å¤ªæ¿€è¿›çš„è£å‰ªï¼Ÿ
        # ä½†å¦‚æœå†å²å¤ªé•¿è¿˜æ˜¯å¾—è£å‰ªã€‚build_context æ˜¯å®‰å…¨çš„ã€‚
        inputs = context.build_context(state, prompts.SUMMARIZER_NOTE_PROMPT)
        
        response = model.invoke(inputs)
        summary_text = response.content
        
        # 2. è®¾ç½®é€€å‡ºæ ‡å¿—
        return {
            "summary_output": summary_text, 
            "should_exit": True
        }
        
    # åœºæ™¯2ï¼šReview (ä¸´æ—¶å›é¡¾)
    elif plan and plan.request_summary:
        inputs = context.build_context(state, prompts.SUMMARIZER_REVIEW_PROMPT)
        response = model.invoke(inputs)
        return {"summary_output": response.content}
        
    return {}

def aggregator_node(state: AgentState) -> Dict[str, Any]:
    """
    æ±‡æ€»è€…ã€‚å°†æ‰€æœ‰ Worker çš„è¾“å‡ºèåˆæˆæœ€ç»ˆå›å¤ã€‚
    """
    tutor_out = state.get("tutor_output") or ""
    judge_out = state.get("judge_output") or ""
    inquiry_out = state.get("inquiry_output") or ""
    summary_out = state.get("summary_output") or ""
    
    final_response = None

    # åœºæ™¯: Ending or Normal
    if state.get("should_exit"):
        # å¦‚æœå¤„äº Concluding çŠ¶æ€ï¼Œç›´æ¥ä½¿ç”¨ Summary Output
        final_response = AIMessage(content=summary_out)
    elif not any([tutor_out, judge_out, inquiry_out, summary_out]):
        # è¿™é‡Œå¢åŠ ä¸€ä¸ªå¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ‰€æœ‰è¾“å‡ºéƒ½ä¸ºç©ºï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œåšå…œåº•
        final_response = AIMessage(content="æˆ‘ä¼¼ä¹æ²¡å¬æ‡‚ä½ çš„æ„æ€ï¼Œèƒ½å†è¯´å…·ä½“ç‚¹å—ï¼Ÿ")
    else:
        # æ„é€  Prompt
        prompt = prompts.AGGREGATOR_SYSTEM_PROMPT.format(
            tutor_output=tutor_out,
            judge_output=judge_out,
            inquiry_output=inquiry_out,
            summary_output=summary_out
        )
        
        # æ±‡æ€»è€…ç›®å‰ä¸éœ€è¦å¤ªé•¿çš„å†å²ï¼Œåªçœ‹å„ä¸ªæ¨¡å—çš„è¾“å‡ºå³å¯ã€‚
        # ä¼ å…¥ System Prompt å³å¯ã€‚
        # ä¸ºäº†è¯­æ°”è¿è´¯ï¼Œä¼ å…¥æœ€è¿‘ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¹Ÿæ˜¯å¥½çš„ã€‚
        inputs = [SystemMessage(content=prompt)]
        if state["messages"]:
            inputs.append(state["messages"][-1])
            
        final_response = model.invoke(inputs)

    # ---------------- å­˜æ¡£ä¸å‹ç¼©é€»è¾‘ (Auto-Save & Compress) ----------------
    # æ¨¡æ‹Ÿâ€œçŠ¶æ€æ›´æ–°ä¹‹åâ€çš„æ•ˆæœï¼šæˆ‘ä»¬éœ€è¦æŠŠæœ€æ–°çš„ AI å›å¤åˆå¹¶è¿›å»æ‰èƒ½å­˜åˆ°å®Œæ•´çš„è®°å½•
    
    current_messages = state["messages"] + [final_response]
    
    # 1. åˆ›å»ºå³æ—¶å¿«ç…§ (ç”¨äºè®¡ç®— Memory)
    temp_state = state.copy()
    temp_state["messages"] = current_messages
    
    # 2. æ‰§è¡Œå†…å­˜å‹ç¼© (Maintenance)
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©æ—§æ¶ˆæ¯
    new_summary, new_cursor = context.manage_memory(temp_state)
    
    # 3. æ›´æ–°è¦ä¿å­˜çš„çŠ¶æ€
    state_to_save = temp_state.copy()
    if new_summary != state.get("conversation_summary"):
        # å‘ç”Ÿäº†å‹ç¼©ï¼Œæ›´æ–°çŠ¶æ€
        state_to_save["conversation_summary"] = new_summary
        state_to_save["summarized_msg_count"] = new_cursor
        
        # (Optional) Print debug info
        rprint(f"[dim]   ğŸ§  Memory Compressed! New summary length: {len(new_summary)}, Cursor: {new_cursor}[/dim]")
    
    # 4. æ‰§è¡Œç‰©ç†å­˜æ¡£
    memory.save_session(state_to_save)
    # ----------------------------------------------------
    
    # è¿”å›ç»™ Graph çš„æ›´æ–° (åŒ…æ‹¬ messages å’Œ å¯èƒ½æ›´æ–°çš„ summary/cursor)
    return {
        "messages": [final_response],
        "conversation_summary": state_to_save["conversation_summary"], # å¯èƒ½æ²¡å˜
        "summarized_msg_count": state_to_save["summarized_msg_count"]  # å¯èƒ½æ²¡å˜
    }


# --- 3. Edge Logic (æ¡ä»¶è·¯ç”±) ---

def route_from_analyzer(state: AgentState) -> Literal["tutor", "judge", "inquiry", "summary", "aggregator"]:
    plan = state.get("plan")
    if not plan: # Should not happen
        return "aggregator"
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœéœ€è¦æ€»ç»“æˆ–é€€å‡ºï¼Œä¼˜å…ˆè·¯ç”±åˆ° Summary
    if plan.request_summary or plan.is_concluding:
        return "summary"
        
    if plan.needs_tutor_answer:
        return "tutor"
    elif plan.needs_judge:
        return "judge"
    elif plan.needs_inquiry:
        return "inquiry"
    else:
        return "aggregator"

def route_from_tutor(state: AgentState) -> Literal["judge", "inquiry", "aggregator"]:
    plan = state.get("plan")
    if plan.needs_judge:
        return "judge"
    elif plan.needs_inquiry:
        return "inquiry"
    else:
        return "aggregator"

def route_from_judge(state: AgentState) -> Literal["inquiry", "aggregator"]:
    plan = state.get("plan")
    if plan.needs_inquiry:
        return "inquiry"
    else:
        return "aggregator"
        
def route_from_summary(state: AgentState) -> Literal["aggregator"]:
    return "aggregator"

# --- 4. Graph Construction ---

def build_agent():
    builder = StateGraph(AgentState)
    
    # Nodes
    builder.add_node("analyzer", analyzer_node)
    builder.add_node("tutor", tutor_node)
    builder.add_node("judge", judge_node)
    builder.add_node("inquiry", inquiry_node)
    builder.add_node("summary", summary_node)
    builder.add_node("aggregator", aggregator_node)
    
    # Start -> Analyzer
    builder.add_edge(START, "analyzer")
    
    # Analyzer -> ? (Waterfall Flow)
    builder.add_conditional_edges(
        "analyzer",
        route_from_analyzer,
        {
            "tutor": "tutor",
            "judge": "judge",
            "inquiry": "inquiry",
            "summary": "summary",
            "aggregator": "aggregator"
        }
    )
    
    # Tutor -> ?
    builder.add_conditional_edges(
        "tutor",
        route_from_tutor,
        {
            "judge": "judge",
            "inquiry": "inquiry",
            "aggregator": "aggregator"
        }
    )
    
    # Judge -> ?
    builder.add_conditional_edges(
        "judge",
        route_from_judge,
        {
            "inquiry": "inquiry",
            "aggregator": "aggregator"
        }
    )
    
    # Summary -> Aggregator (Always)
    builder.add_edge("summary", "aggregator")
    
    # Inquiry -> Aggregator (Always)
    builder.add_edge("inquiry", "aggregator")
    
    # Aggregator -> End
    builder.add_edge("aggregator", END)
    
    return builder.compile()

